# Cache Design

该文件并非仅仅描述Cache主体的结构，而是记录了开发Cache的总过程，以此我们试图构造一个比较细致的开发指导。因为Cache的开发参考较少，但是Cache对于性能提升比较关键，因此我以这种方式来给读者提供便利，加速Cache的开发过程。本文档介绍了绝大部分的实现细节，当然最具体的细节存在于代码之中。

提醒：本Cache所适应的CPU架构为双发射架构，具体内容可见主目录报告。直接搬用本内容很有可能会导致不适配读者设计的CPU。

### 作者

李程浩，哈尔滨工业大学（深圳），2020届龙芯杯UltraMIPS团队队长。loancold@qq.com

### 基本设计概览

本部分，我们直接给出了最终版本的设计细节。

#### Cache配置

全流水（两级），8KB，二路组相联，伪LRU替换法则。DCache对写采用Write-Back与Write-Allocate策略。拥有WriteBuffer加速DCache写主存，CacheAXI_Interface做总线交互仲裁。动态取指结合分支预测充分爆发CPU的双发射性能。

#### 地址编码

高速缓存的设计为8KB的二路组相联，块大小设为32个字节，即8个字，每个字为一个bank，首先给出以下的32位物理地址编码规则。

- Tag位（物理地址高位）：20位		Index位（组地址）：7位		Offset位（块内偏移）：5位

对于额外的标志位，有以下的设定。

- Valid位（数据是否有效）：1位		Dirty位（脏标志）：一位

每一组有两路，共用一个LRU位。

- LRU位（最近最少替换标志）：1位

脏标志是只有在弹出的时候才会使用，与Valid和Tag的使用方式区别较大，所以将其分开，最终，块的数据编码如下：

| Tag+Valid | Bank0 | Bank1 | Bank2 | Bank3 | Bank4 | Bank5 | Bank6 | Bank7 | Dirty |
| :-------: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
|   20+1    |  32   |  32   |  32   |  32   |  32   |  32   |  32   |  32   |   1   |


#### 主存写策略

当Cache写入主存中更新其数据的时候，采用的方式是写回方式，需要有脏数据的标记。当执行主存写指令的时候，若其数据存于Cache之中，则将Cache中的数据进行更改，并且同时将其所在块标记为“**脏**”。等到进行Cache数据替换脏块的时候，将脏块写入进内存中。此策略被称为Write-Back。

但是当Cache中没有指定的数据，那么首先从主存中读取指定的数据，再将其标记为脏。该策略被称为Write Allocate。

#### Cache数据替换策略

当Cache未命中的时候，或者说需要从主存中读数据写入Cache的时候，采用的策略是伪LRU（伪近期最少使用法），对于2路组相联，对每一组（set）都设置了一位的LRU标志，为0表示way0最近没有被使用，为1同理。LRU位决定了替换该组中的块时被替换的那一个。注意，被替换的数据若是dirty的话，就必须写入内存。

#### 整体的处理流程

为了增加器件复用的程度，设计中的读写整体处理流程都是追求一致的，具体如下，作者第一次知道此思想来自《超标量处理器设计》，但是实际上所有书都有（害，千篇一律的Cache）。



### 硬件架构设计

#### ICache接口设置

输入：

- 全局信号：时钟clk，复位rst
- CPU指令读写命令：读使能cpu_req_i，虚拟地址virtual_addr_i
- CPU暂停控制：stall_i

输出：

- CPU指令读命令结果：是否命中cache hit_o，当前数据是否有效 inst_valid_o，当前输出的数据cpu_inst_o
- CPU控制信号：取数据暂停信号stall_o，取值结果只支持单发single_shot
- 总线slave信号：读数据有效mem_rvalid_i，读地址可以接收mem_arready_i，主存送入的块数据mem_rdata_i
- 总线master信号：读使能mem_ren_o，读数据可以接收mem_rready_o，读地址有效mem_arvalid_o，读地址mem_araddr_o

注意：

- hit_o：在req_i给出的下一个周期给出，高电平表示命中，低电平表示不命中

#### DCache接口设置

输入：

- 全局信号：时钟clk，复位rst
- CPU指令读写命令：读使能cpu_rreq_i，写使能cpu_wreq_i，虚拟地址virtual_addr_i，写数据cpu_wdata_i

输出：

- CPU指令读命令结果：是否命中cache hit_o，当前数据是否有效 inst_valid_o，当前输出的数据inst_o
- 总线slave信号：读数据有效mem_rvalid_i，读地址可以接收mem_arready_i，主存送入的块数据mem_rdata_i
- 总线master信号：读使能mem_ren_o，读数据可以接收mem_rready_o，读地址有效mem_arvalid_o，读地址mem_araddr_o

注意：

- hit_o：在req_i给出的下一个周期，该信号有效

#### Cache_Ram的构造

为了存放相应的数据，对于每一路构造以下结构的Block Ram：

- Tag+Valid：21位*128块\*2路（实际为了对齐8位采用32位，第21位为Valid）
- Bank0~7：32位*128块\*2路

对于脏位和LRU则采用寄存器直接构建：

- Dirty：1位*128块\*2路
- LRU：1位*128块

关于地址，由Index指定块的深度（也就是组序号），由Offset指定块内偏移，下面具体明确与地址的对应关系：

- Tag+Valid,Dirty的ram地址:Index
- Tag+Valid,Dirty的ram选择使能:永远为真（两路全部取出）
- Bank的ram地址：Index
- Bank的具体使能：用B<sub>i</sub>W<sub>j</sub> 表示第i个bank，第j路，则有Offset[4:2]指明第i个bank，两路使能相同。
- Dirty的ram地址：Index。

对于TagV ram，其中写入的数据永远是物理地址，只要将写入数据段与物理地址相接就可以了。

在非流水Cache中，对于ram来说，不会发生同时写和读的情况，但是在流水Cache中则会发生。ram必须直接组合逻辑连接于输入的addr，以保证一周期反应，那么在非流水Cache中就需要进行输入addr的选择，以保证能写能读。在流水Cache中，则需要使用多端口的ram同时进行读写，同时注意在同时进行读写的时候，如果两端口地址相同，输出的读数据将直接使用写入的数据（这部分需要自行实现），这部分是IP核自身的collision问题，需要进行规避。（顺带提醒：simple-dual ram完全等价于只开放A的write和B的read的true-dual ram）

#### Cache读写规则

##### 主存读命令：命中Cache

- 首先，将用TLB换算出来的物理地址拿出。
- 经过地址寻址，交付给cache_ram取出对应的数据。
- 进行对比确定是否命中，命中则返回。

##### 主存读命令：未命中Cache

- 读出ram的数据如果是脏块的话不要丢弃，要送给WriteBuffer（见后文详细介绍）进行写入。
- 相应地址使能等交付给总线，之后开始等待。
- 数据返回后送出，同时将数据送给ram进行写回。

##### 主存写命令：命中Cache

- 首先，将用TLB换算出来的物理地址拿出。
- 经过地址寻址，交付给cache_ram取出对应的数据。
- 进行对比之后，将拿出的数据进行重写。
- 将数据进行写回。

##### 主存写命令：未命中Cache

- 首先，将用TLB换算出来的物理地址拿出。并且同时用虚拟地址地位赋予ram取数据，同时将物理地址给FIFO。
- 如果发现ram和WriteBuffer都未命中，将相应地址使能等交付给总线，之后开始等待。等待的时候如果不是马上要处理另一条访存那么可以让CPU先行。
- 数据返回后与写数据拼接，同时将数据送给ram进行写回，若有脏块，弹出至WriteBuffer。

#### WriteBuffer: FIFO

该部件实质上是一个写总线的队列，用来单独与总线进行数据交互的，以解放DCache。作为写缓存，避免了流水和cache的堵塞，提高性能，但是要注意与cache的联合会出现以下的功能问题。

等待写回的数据在FIFO之中。若在写回之前，对于队列中的数据又出现了读/写操作，则需要将相应的数据抽离出来（这里头的才是最新数据，否则会出现数据错误），这不但可以加速读写的进程，还可以减少总线和FIFO的压力。值得提出的是由于是以块为数据单位，那么地址位的比较就要做一定的额外处理。为了保证尽可能的低耦合性，对于任意的地址输入输出，都将Offset置0。

WriteBuffer存满后再接受块的时候是**不会拒绝**的，因此会出现溢出，这部分需要由DCache进行管控，在遇到这种情况的时候进行暂停。

#### CacheAXI_Interface

由于Cache送往总线虽采用了握手，但是考虑到功能实现以及其模块化处理，Cache最终以块为单位数据与总线进行沟通，因此我们还需要设置一个中间的Interface，也就是该模块的目的。该模块将使得Cache能够忽略与总线交互时所发生的猝发以及具体的交互信号，将重心放在核心逻辑上，避免无关代码扰乱视听。同时，总线的数据访问不接受同时读，因此我们需要处理ICache和DCache的读冲突（写不会冲突）。我们的处理策略是，访存优先级大于取值。

该模块将块拆成多个字一个一个送入总线之中（写），也可以将总线传回来一个一个字进行块组装，再送回总线中（读）。同时考虑到读冲突，我们要设立优先级。



### 第一代Cache

第一代Cache将给予一个半成品的Cache，即可完成大部分的功能，以及接近完整的接口，但是功能性没有办法得到保障。同时，为了一步一个脚印进行迈步，首先我们设计的Cache为状态机Cache，没有办法在命中的情况下解决连续访问请求。

#### 状态机设计

##### ICache状态机

首先，采用状态机而非流水对对应的模块进行开发，读数据状态分配如下：

- Look Up: 将地址传给TLB进行虚实地址转换，并将该地址给予相应的RAM。下一阶段进入Scan Cache
- Scan Cache: 取出对应的cache数据，判断是否命中，将命中结果传出，若命中将数据也传出。命中后跳转到状态Look Up，没有则跳转到Read Fail。
- Hit Fail: 等待至数据返回，（传递数据和地址给ram，下个周期才会写回），然后返回状态Look Up。
- Write Back:拿到总线的数据之后直接将CPU需要的对应的数据送出，同时将对应的数据写回至cache中，下个周期返回状态Look Up。

##### DCache状态机：

- Look Up: 将地址传给TLB进行虚实地址转换，并将该地址给予RAM。下一阶段进入Fetch Data
- Fetch Data: 取出对应的cache数据，判断是否命中，若**读命中**,进行输出，之后跳转到Look Up，若**写命中**，设置好数据送入ram进行写入，之后进入Look Up。若**读不命中**，则与总线交互等待数据，之后进行关键字输出，跳转至Look Up。若**写不命中**，同读不命中，但是写入数据必须考虑到cpu要求的写数据。若有脏块，进入Write Data。
- Write FIFO: Fetch Data所获取的数据（写则需要加上cpu写数据的修改）写入FIFO。
- 对于不同阶段，都有固有的组合逻辑器件进行操作，状态机只相当于“流水线”中间分割的寄存器，但是为了功能的实现，目前的状态机不能够直接切分成流水。

##### 写ram的使能需要在以下情况打开：

- 写命中：将命中位置高位
- 读写不命中：在返回数据的时候将被替换位至高

##### 将写入ram放在Fetch Data的最后一个周期，其中写数据的来源有如下情况：

- 读未命中：总线数据
- 写未命中：总线数据与脏块拼接
- 写命中：分0路、1路、FIFO命中三种，分别拼接数据
- 对于LRU的变换法，其中的数据在以下情况需要变动：
- 非FIFO命中：LRU指向没被命中的块
- 未命中：由于主存取块覆盖在了原来的LRU指向位置，于是将对应位置的LRU取反即可，表示下次覆盖块为另一块。

##### 对于脏标志的设置，在以下情况需要变动：

- 读未命中：对应替换块脏位设0
- 写未命中：对应替换块脏位设为1
- 写命中但是不是FIFO命中：对应替换块脏位设为1（该情况要注意）

##### 对于主存写入，在以下情况会出现：

- 读未命中：若弹出块（LRU决定）为脏块，则送入FIFO（使能也要有效）

- 写未命中：若弹出块（LRU决定）为脏块，则送入FIFO（使能也要有效）

- 写命中FIFO（特别注意）：若写入的块在FIFO中，那么我们就需要将写入块送入FIFO(如果过了一个周期，原来取得的块写入到主存中去了，也没有关系，FIFO会将其加入队尾等待写入)

##### 写入FIFO的数据来源：

- 写命中FIFO：此时将FIFO读出命中的块与写数据合并送入FIFO再次进行覆盖写入。
- 脏块弹出：将LRU_pick指向的数据送入FIFO写入，但是要注意的是AXI取出的输出写入了ram之后，脏块就丢失了，因此需要在AXI取数数据（bus_read_success）的时候将ram中的块拿出来，等待时钟沿的来到。

##### WriteBuffer 冲突：

特别地，对于WriteBuffer，我们注意到写替换命中会出现冲突，当写命中的数据正在被写入总线，就可能会使得我们想写入的数据没有进入总线。我们采用的处理策略是：读写命中分开判断，但是判定逻辑很相似，即读命中且头指针所在位没有命中可以表示写命中。

Read/Write Double Hit Collision：读写双命中意味着FIFO中有两个相同地址块，很明显其中一个正在被写入，这种情况下系统相会失去判断能力甚至给予错误的数据。这代表WriteBuffer存在着根本性的功能错误，是失败的产品。这种情况的处理会加大其内部的复杂度，我们会在第二代Cache进行重新设计。



### 第二代Cache

首先需要进行说明的是，Cache第二代是从半成品迈向成品的全新的一代，它拥有主体部分、完整的WriteBuffer、CacheAXI_Interface，不仅如此，在功能上它成为了完整的Cache产品，能够满足具体的使用，我们可以预见，alpha第二代将会迈向beta第一代，而这一代不仅将在功能上表现全面，还会在优化上稍有建树，包括有关键字处理技术、设计精良的三状态式Cache，仅仅400行代码的DCache主体等。在提出美好的愿景之后，我们仍然面临着许多需要修改的内容，下面对现在已经实现的结构已经将来需要实现的结构做一定的说明。

##### 关键字预取（不实现）：

在总线的块读取过程中，一旦发现这是我们需要的字，立刻取出送给Cache。但是这里需要的改动需要在cpu中进行对应结构的改变，因此，我们首先不考虑这样的功能，我们会在功能测试之后进行相关的开发。同时，我们注意到，实际访存周期超过500，剩下几个周期其实并没有这个必要，而且还增加了设计的难度，因此最终我们不打算实现。

##### 信号的优化（已实现）：

显而易见，为了考虑完整的总线接口请求，在第一代的Cache之中，我们使用的是AXI总线的naive共同协议，但是在Interface的加持之下，我们似乎可以对其进行sram化，更为精简，只保留最基本的使能和AXI操作完成标志。

##### CacheAXI_Interface（已实现）：

写和读不冲突，但是inst和data的读会冲突，因此，我们采用类似于状态机的写法进行状态的跳转，在遇见inst 和data同时做请求的时候，我们将会给予data以优先权。我们知道，写不会出现busy的状态（FIFO会处理好这一切），但是读会出现。对于读冲突，我们之后会使用关键字技术，则不会等到完全读主存结束才放行Cache，因此我们之后需要单独判断暂停位告诉Cache我们还没有完成，但是现在，我们暂时不需要。

##### 暂停（已实现）：

对于CPU来说，Cache读不命中时和写访存堵塞（同时处理写指令）的时候，需要进行暂停，等待Cache结束相关操作。对于Cache来说，需要解决写访存堵塞（同时处理写指令）的状态调整。

CPU会在**执行**阶段给DCache发送访存相关信息，然后Cache会在**访存**阶段送回数据（顺利的话），但是现在单发射的逻辑是访存阶段给予访存相关信号，这就意味这同时需要进行单发射CPU的逻辑改正，否则会拖累性能。下面我们还是考虑**执行**阶段给予访存信号的情况。

DCache暂停时，即两周期解决不了读写指令，有两种情况，一种是cpu读指令读不命中需要等待，一种是写冲突。特别对于写冲突需要进行说明，由于写不需要返回给CPU数据，可以让CPU先行，写命令和读不命中写脏块自己执行，但是其中会遇到问题：写脏块中途cpu又提出了访存的要求，FIFO满了的时候当前又出现了没有hit的脏块写入（Write Data状态）

综上，暂停可以用状态表示：

- 非Look Up情况，不接受任何CPU的req，发现就堵塞

- Fetch Data：当该状态没有hit的时候，是命中失败了，对于读肯定是要堵塞，但是对于写就不必要了。

- Write Data：这个状态一定是未命中（写脏块）的状态，但是由于是写入FIFO，出现FIFO堵塞就要设置暂停。

##### WriteBuffer重构（已实现）：

在之前的WriteBuffer之中，我们采用的是重复写入进行写命中正在写入块(之后简称为**写冲突**）进行冲突处理，但是我们会遇见非常棘手的问题，因此我们将重新审视我们的代码，进行重构。

为了根本上解决已有问题，我们将不再进行重复写入，而是覆盖，保证FIFO中不存在同地址的新老数据。但是由于写操作猝发不能够中断，因此遇到**写冲突**的时候我们需要等待第一次写完之后再重新写一遍。因此我们需要重新加入一个标志位来解决这种问题。

##### 写冲突（Write Collision）（已解决）：

当写操作命中队头的时候，将设置重写标志（rewrite），这个标志将会使得FIFO的队头在写完之后不移动，使得再重写一遍。要注意设置相关信号：valid和head在rewrite为高位的时候保持自身的valid。

当队头刚好写完的时候（bvalid为真），传入了一个write collision，这个时候从理论设计上不进行复写。实际上由于之前write_hit在队头处为真下一周期会出现：rewrite依然为低电平（正确），头指针后移（正确），头指针处设为invalid（正确），尾指针不后移（错误），队头保持valid（错误！）

所以采取修正措施，让其复写。使得下一周期会出现头指针不后移，并保持valid。

### 第三代Cache

第三代Cache主要关注的问题是取指部分的加速，包括“流水”和更为先进的优化技术，我们将着手进行让ICache充分发挥其性能。具体的任务有：命名规范化、ICache流水化、ICache 指令预取（FIFO）、Interface关键字技术。同时对于功能性要求，我们也需要进一步进行满足，使得外设的存取功能正常。注意，我们现阶段对data和inst都需要做Uncached的相关判定。

当前的DCache是非常普通而平凡的，如果有时间的话我们希望能在第四代Cache中加入一些先进的内容：DCache流水，双访存，victim buffer， uncached write buffer等。

#### Uncached映射（实现）

在对外设进行访存处理的时候，不能够进行缓存处理，必须立刻执行。但是对于写，我们在此做一个保留，或许晚点写也不会出现很多问题，因此可以考虑Buffer。具体来说，对与Uncached，其信号由TLB给出，那么我们需要将TLB移至Cache外层。对于外层Cache（也就是inst和data以及interface的顶层），我们要能够处理这样的Uncached信号，并且做出处理。对于Interface我们也要处理这种Uncached的非猝发单字传输。下面我们给出具体的解决方案。

##### Cache顶层模块设计

Cached和Uncached本质上是一个选择，我们直接进行组合逻辑assign实现。

在该模块中，我们首先对uncached进行判定，若成功给予DCache访问req低电平，之后设置对应Interface的uncache data访问高电平，并将相关数据进行送入，在等待的结果时，我们也要进行相应的暂停操作的设定，以及完成相应操作的输出。对于Inst，我们也进行相同的操作。

需要进行留意的是，我们对ICache附加了一层流水线清空的信号，对于uncached inst处理也进行了相关操作。对于数据的对应valid信号，我们暂时不进行更改，设置其在uncached情况下不使用，这是因为我们对cpu进行了stall信号的处理，不需要再考虑valid信号，我们会在后期考虑其必要性。

对于访存暂停，要分为两类，如果是读，那么暂停就来自于cached和uncached，如果是写同理，最终的暂停是两者的或。

##### CacheAXI_Interface设计

新增3个通道：指令读、数据读写不命中通道。同时也新增相应的状态，进行读写仲裁。但是实际上ICache总线读请求和inst uncached读请求不会同时发生，这样我们可以复用一些部件。但是读数据和写数据会出现冲突，对于读数据，当DCache执行写指令不命中时会放行流水线，此时会读访存，如果在其间遇见了uncached读信号就会出现冲突，没有办法复用通道。同时对于写数据，由于buffer的存在，也有可能会冲突。

#### Interface关键字技术进一步说明（不准备实现）

Interface在与主存进行操作的时候，如果获得了对应字的数据则先行输出，不等整块读出再从块中进行选择。但是在具体来说，实现的时候会出现一些问题。注意该技术对于DCache更有用（访问频度不高），但是对于ICache（持续访问），即使先行送出，但是块打包的过程并未结束，就算提前送出，下一条指令也会收到影响无法流动。

ICache：ICache进行主存读取的时候，注意到Interface使用了一个块的寄存器一个字一个字的接受总线返回的数据，那么我们先假设要取块中第五个指令。在总线取出第五个字的时候，我们将其输出，由于是猝发，总线将每一个周期送出一条指令，那么我们流水ICache自然也可以持续的将第六个第七个第八个指令送出。在这种处理之下，Interface的总线块寄存器也可以被当作是一个缓存，在流水畅通的情况下，我们可以让时间都花在访存上，预计可以节省10-20%的指令访存时间。但是我们还注意到，我们还会加入指令预取技术，这样的技术实际上会降低缺失率，这使得关键字技术的收益降低，甚至是无效。但是目前我想到了一种合二为一的结构，我们将在**指令预取**部分进行具体介绍。

DCache：由于Interface的从总线读数据步骤统一，对于DCache和ICache都没有差别，因此关键字技术应该可以以相同的方式惠及两者。需要注意的是，DCache不流水也可以使用这样的方式获得比较高的收益。由于访存指令并非同取指一般频繁，因此一般不会出现访存指令连续的情况，所以可以取得比较好的收益，但是实际上我们不能期待这样的性能能够做到非常大的提升，事实上可以预测这只是九牛一毛。整体应该能够提升1-2分性能分。

具体的技术构造将会在流水实现之后给出。

#### InstBuffer：FIFO

在双发射流水线的支持下，我们需要对于单发和双发进行适配，由于流水线的延迟关系，pc没有办法及时更新，其中就会出现些许错位以及空隙，对于这样的空隙，我们加入一个InstBuffer，即简单的FIFO。由于FIFO的实现，发射模块可以自由的进行指令处理。同时，FIFO让空隙进一步扩大，也就是解耦合，从此开始取指与流水线执行部分已经几乎彻底分离，这样好处多多，能够解放ICache不被cpu限制进行指令预取，也为动态取指坐下了铺垫，接下来介绍FIFO的具体设计。

##### 出队逻辑

发射模块将会给予FIFO的出队信号，进行指令的取出，同时发射模块会给予单双发信号，若单发则头指针移动一格，双发两格（注意：如果Buffer只有一条指令但是进行了双发射，就会导致错误，这需要发射模块遵守规定）。由于队列是组合逻辑设计，那么可以立即取数，即两条指令对于发射阶段是一直可见的。显然，整个InstBuffer不一定始终有足够的内容进行取出，那么我们将会对发射阶段进行指示，让其执行空指令，但是空指令的部分需要由发射阶段实现。为了防止出现跳转指令在其中但是延迟槽指令不在的情况，我们要预留发射的指令，达到一定的数量（发射阈值）才允许发射。

值得注意的是，会出现跳转指令发射后延迟槽指令虽然在InstBuffer中但是低于发射阈值不允许发射的情况，这个时候需要

##### 入队逻辑

入队由pc进行指令取出控制，我们将一次取出两条指令，但是由于块边缘限制，我们会遇见只能取出一条指令的情况，这需要尾指针只移动一格。显然，边缘情况pc可以进行自判断。我们注意到，整个FIFO会面临满状态，在这种状态的时候，我们将会指示pc不进行变化，也就是不再取指。由于取指需要时间，之前已经取的指令也不好收回，所以我们要在将要满的时候设发出停止取指信号。当然，对于这种入队情况我们也要保持尾指针不动，其他的情况我们让pc一次移动两格指令。

##### 清空逻辑

当我们遇到rst或者是流水线清空的信号（也就是跳转指令），我们要将Buffer中的数据进行重置。

#### ICache流水

在这一代的ICache中，在实现了双发射的基本适配的情况下，我们再次准备向前迈进一大步。同时我们还要解决相应阻塞问题，让ICache能够连续进行流动，同时我们还要做到指令预取以及双发射功能中的FIFO适配，用以解决边界单发射问题以及在流水过程中pc和单双发指令不匹配问题。

##### ICache流水切分（已经实现）

ICache的流水沿袭了之前的状态机设计，共三级流水线，但是实际上只有两级参与流水，这样我们就能够在命中的时候连续的从cpu的取指阶段接受读指令信号，同时在译码阶段不停的送回指令数据。如果之后深流水线可以变为三级取值，那么之后需要再次进行流水切分设计，将当前并行变为串行。

- LOOK_UP（流水）：将数据赋值于ram取数据，同时这个阶段要处理不命中返回ram的写入操作（该操作会造成数据相关问题）。
- SCAN_CACHE（流水）：对读出的数据进行处理，判断是否命中，命中则输出，不命中则送入不命中额外处理。
- HIT_FAIL（不命中额外处理）：访存，在访存结束的时候输出数据，并且将读出块写回ram。

##### ICache流水控制器（已经隐式实现）

流水控制器是用来解决不命中情况下的流水线情况，合理的计划流水线的流动。对于ICache，我们简单的将其考虑为两个状态，这两个状态为组合逻辑转换。

- WORKING：流水线运行正常，没有不命中发生，检测到不命中进入LOADMEM。这个时候不发出流水线暂停信号，并且HIT_FAIL部分不进行操作，指令输出为SCAN_CACHE级的数据。
- LOADMEM：当前正在处理流水不命中情况，，检测到结束之后进入WORKING。这个时候发出流水线暂停信号，并且将ICache流水线进行暂停。指令输出为总线方给予的数据。

##### ICache暂停机制

在ICache非流水的5级双发流水线cpu中，我们巧妙的采用了两拍之后的ex跳转flush与rst看作同一信号进行清空，使得状态机能够刚好在访问主存之前复位，这使得ICache不会访存出错。但是，在流水ICache和InstBuffer的加入下，跳转的时候ICache的状态变为了不定态，因此，我们要对ICache进行一定的控制，能够让其在访存时收到flush之后能够等待访存结束之后才正式的开始工作。具体设计如下。

- 在Cache模块（也就是顶层模块），设计相应的状态机，当遇到flush时且ICache进入了访存状态，则进入ICache的持续flush状态，当访存成功，则撤销该状态。
- 在flush的当拍，无论ICache做了什么操作，由于InstBuffer会复位，所以当拍不给予ICache复位信号。
- 在处理ICache进行flush的同时，我们应该也要送出暂停信号，使得pc也保持不变。

##### ICache结构

对于Cache Ram，我们将其设置为读写双端口，并且对于命名和结构进行调整，先将其适应于非流水状态机ICache，需要注意，采用simple dual port ram，读写同地址进行访问的时候，写优先，下一拍读出的数据为错误数据。

为了解决读写同地址冲突，我们做一个数据冲突的管理，结构如上。具体的原理如下：

- BankRam：读写冲突的时候，也就是不命中总线返回数据的时候，我们将总线返回的数据停一拍，然后下一拍再用一个选择器来判断是否出现了**冲突**，并据此选择数据。这个选择器的冲突判断信号是上一拍发生的，也要停一拍。
- TagRam：结构同理。

#### 动态取指

由于当前的pc的控制能力已经减弱至仅仅为取指，ICache和InstBuffer将进一步控制取指阶段的操作，因此，分支预测也就不得不接入到整个取指体系。同时，我们会遇到由此相关的pc取指问题，因此我们将接管一部分npc的功能，计算出npc，这表示pc已经大部分成为了ICache控制器的一部分。

分支预测能够判断该pc是否为跳转指令，并且是否跳转，跳转的地址是多少，ICache将按照该预测进行取指，以保证在跳转的情况下，延迟槽以及跳转之后的指令能够被正确取出。当然对于不跳转也要正确处理。为保证FIFO中的所有指令都要被执行，具体的动态取指设计如下。

##### 取指规则

动态取指需要控制后两次取指，因此需要用状态机，当取指结束的时候更新状态而不是时钟沿。

第一条指令将会跳转：取两条指令，之后开始取跳转之后的指令。

- 遇见边界问题：控制ICache取两次指令，npc=pc+4。下一拍进行跳转pc=pc_b，取第二次指令的时候第二个指令失效，只有第一个指令有效
- 非边界问题：控制ICache取一次指令，npc=pc_b。

第二条指令将会跳转：取三条指令，之后开始取跳转之后的指令

- 遇见边界问题：控制ICache取两次指令，npc=pc+4。下一拍进行跳转pc=pc_b，取第二次指令的时候一定不会遇见边界，也就一定可以取出。
- 不遇见边界问题：控制ICache取两次指令，npc=pc+8。下一拍不管遇不遇见边界问题，都跳转npc=pc_b，并且令第二个指令失效，只有第一个指令有效。

两条指令都不跳转：正常取指

可优化：

- 为加快取指，可以设置遇见跳转问题的时候多取指令，至多取出4条。
- 现在我们已经预测到了分支指令，那么就没必要等待InstBuffer存储有三条指令得时候才允许发射，我们可以允许提前发射。
- 现在取指一次最多两个指令，发射也是最多两个指令。这样难免会出现发射等待取指的情况，阻碍了流水线的流动。我们可以试图一次取更多指令，以解决这种问题。

##### 时序控制

由于ICache已经实现了流水化，因此分支预测将会出现时序上的延迟，也就是当前取出的指令与分支预测当前判断的指令不同，因此整个动态取指的控制（下图曲线）应该也要延迟一拍才能够控制其进入InstBuffer，使得cpu能够正确的读出指令。在最终送给InstBuffer的信号中，我们将动态取指的控制信号与ICache的数据有效信号做与运算，也就是控制信号为控制ICache的数据有效信号输出使能。

注意，ICache取指不命中的时候，要让控制信号也要同样的保存起来，保持不变。



#### Uncached重新设计

在之前的妥协五级流水双发射中，由于设计的问题，我们每次访存命中会阻塞一个周期，现在我们将去除这样的问题，但是我们将会带来新的挑战，也就是uncached的适配问题。同时，由于流水线ICache的加入，对于指令uncached也显得十分捉襟见肘，因此我们需要重新设计整个Uncached的控制。

##### Data Uncached

对于时序，现有的时序是即刻堵塞，我们需要设立状态机，使其能够满足当前Cached时序，也就是命中不堵塞，不命中缓冲一周期。因此将Uncached的纯组合逻辑改为状态机控制。状态机描述如下：

- DATA_CACHED: 表征cached数据处理。当发现cpu送入的最新数据为uncached的时候（并且DCache的数据已经处理完毕），会在下一周期跳转到DATA_UNCACHED状态，并且当前周期开始进行uncached信号处理，但是可以肯定的是，这种uncached情况不会堵塞。
- DATA_UNCACHED: 表征uncached数据处理，这个时候专注于uncached即可。当发现uncached已经处理完毕，下个周期就跳回DATA_CACHED。

尤其要注意，判断“DCache的数据已经处理完毕”采用DCache_stall信号会出现环路。对于这种情况，换种判断方法，引出DCache状态来进行判定。（图没放，不见了。。。）

##### Inst Uncached

对于取指，在cached部分我们使用了分支预测相关的动态取指，对于uncached段我们应该也要做相关的取指操作。

我们将取指分为cached和uncached两种情况，互不干涉互相排斥，只能同时进行一种取指，我们采用状态机进行控制其取指。

#### DCache时序适应

为了防止阻塞问题，利用访存大多非连续，我们设置cpu在ex送信号，DCache在mem阶段接受信号。但是由于DCache非流水，我们在遇到两条（更多则类似）连续的访存指令将会遇到暂停问题，我们需要在DCache准备好之前给予暂停信号，以保证DCache能够正确的给予相关的反馈。但是，这样会导致一个问题，当DCache准备好接受下一条访存指令的时候，当前处理的访存指令对应的数据也就丢失了。

在实现DCache流水之前，我们打算进行妥协，对此情况直接将其数据单独保留一个周期。

#### WriteBuffer简化（暂时搁置）

实践上，通过性能测试仿真，我们发现实际上WriteBuffer的占用率一直非常低，并且通常保持在一个块。理论上，每一次弹出脏块都是不命中，不命中本身的访存就很耗时，这个时候写入也会利用这段时间完成，因此写入通常不会累积到第二个块。因此，我们可以大幅简化使得WriteBuffer从原来的16个块的容量降低至1个块，这样我们可以将非常多的选择器和加法器等等去除。

这样，我们就会遇见WriteBuffer满的情况，因此需要在DCache进行逻辑的补充。我们为了处理该情况，重新启用了DCache第三个状态STATE_WRITE_DATA，作为FIFO满的等待处理。既然这样，我们就需要进行一个相关处理如下。

- 暂停机制：显然，当我们独自等待buffer空的时候，也就是当前没有新的访存请求的时候，我们仍然和之前一样，不发出暂停。
- 写入buffer数据：对buffer的写入有两种情况，一种是访问主存恰好完成，弹出脏块进行写入（STATE_FETCH_DATA），另一种情况是进入专门等待状态（STATE_WRITE_DATA)。之前，我们解决了前者，现在我们要将数据进行专门保存，使其能够保持至后者，这样我们就可以保证后者的正确数据写入。

#### 数据预取（准备实现Data，最终未实现，但是顺带着实现了动态指令预取，具体见之前具体叙述）

指令预取的基本思路是简单的预测到下面我们会对哪些地址进行访存，我们可以趁命中的时候让访存工作起来，这样的话我们就可以节省访存的时间，这种技术通常可以使得命中率再上一个数量级。本质来说，指令预取的效果是一种trade off，并不一定会使得性能得到增强。

##### 指令预取

但是我们初步的设想是只将其应用至ICache，因为ICache的访存非常直接，程序指令非常大可能是一个循环或者是顺序执行下去，对于前者已经命中，对于后者我们可以很方便的预测到当前指令位于的块的下一个块很有可能使用，因此我们将其预取出来。这种技术也叫做相邻位置预取方式。

我们并不是每次都进行预取，如果我们发现预取的指令块已经在Cache中，那么就没有必要进行预取。但是我们如何在正常的流水之中发现命中呢？好问题。最后发现由于ICache的命中率过高，在正常的单发射cpu中使用指令预取显得非常没有必要。但是却发现，在双发射cpu中我们不仅可以做预取还可以做动态预取，也就是我们之前所提到的动态取指功能。

##### 数据预取（未实现）

由于分支预测是2bit动态预测，而且其预测与数据预取的思想非常相像，也就是用之前的历史状态来预测将来的状态。分支预测采用是否跳转以及跳转的方向预测是否跳转和跳转方向。DCache中的数据预取则是根据历史地址和是否命中来判断将来是否命中，具有较高的相似性。最终因为DCache命中率达到99.59%，所以没有使用该策略。不过读者可以沿着这一思想具体实现，在龙芯杯中可以算一个亮点，即使性能提升不高。

### 第四代Cache

在如此强有力的Cache支撑下，不仅在功能上做到了完备，在性能上也表现出色。现在，我们基本到了比赛的末尾，而在这样的末尾阶段，我们将重心放在了近半个月没有优化的DCache，将其结构进行新的设计，相比于ICache流水的变化，这一次似乎显得更为挑战，同时也具有诱惑性。我们很有信心，在第四代Cache的加持下，性能分数能够再次提升40%（其实DCache流水只提升了8%，但是加上一系列优化提升了28%）。

具体来说，第四代Cache将更注重细节，在时序的优化上做出一些让步，以微小的IPC去换取更好的性能。但是同时，我们又要以退为进，实现DCache流水，实现Cache的大跨步。同时，我们已经意识到，整个DCache已经处于同位非常高的水平，当我们实现流水DCache的时候，也迎来了整个Cache性能设计的终点，也是整场比赛我们所了解到的最高级别的Cache。之后我们将加入完整版TLB和Cache指令，进行功能上的完全版本开发。

总而言之，隐蔽于cpu之后，伫立于AXI之前的Cache显得格外低调，但却毫不低能，以冷酷而简洁的外观示人，却又用强劲的性能令人折服，而这，就是第四代Cache。

#### DRAM DCache（已实现，但未加入最终版本）

cpu的ex阶段在访存指令并没有很多工作要做，只是通过不同的操作类型进行了相应的访存地址的选择。那么我们自然而然可以将其利用起来。于是我们对于Tag的储存将转而使用DRAM，使其能够在状态机的第一个阶段就给出相应的Tag，并且进行命中判断。而在第二个阶段则只需要对数据进行处理即可，尽可能的降低第二阶段的延时，具体的架构如下。

- Look Up: 将地址传给TLB进行虚实地址转换，将虚地址给予TagRAM当拍即读出Tag，并且进行实地址的命中判断。下一阶段进入Fetch Data。
- Fetch Data: 其余的比较与之前BRAM的操作相同，但是要注意存储第一个状态传过来的数据，避免“转瞬即逝”的时序问题。

#### WriteBuffer相关逻辑简化

##### 重写机制（未加入最终版本）

之前，我们为了处理写队头冲突，采用了重写机制，但是这样会造成大量的组合逻辑器件串行工作，因此，我们着手对其进行整体写入冲突机制的改变。

- 读冲突：不变。
- 写冲突：非队头冲突，复写。队头冲突，显示Buffer已满不让写入。

相应的，对于整个DCache，我们也要做拾起遗留问题，即WriteBuffer满的情况。我们为了保持状态机的统一，重新启用第三状态WRITE_DATA，作为写入WriteBuffer的专用状态。

这一次优化的最终结构是：失败。刀法不准，切错了地方。

##### WriteBuffer读取切分

我们发现，在经过一层TLB虚实转换之后，需要送给WriteBuffer判断是否命中，然后将数据取出，但是这样我们经过的处理过于复杂，更不用说之前执行部件中的相对寻址还需要进行相应的32位加法。因此我们尝试了两次方法，最后一次获得了相当显著的成果，频率上升至87MHZ，提升了11%。

- 将WriteBuffer的命中处理延后至状态机的第二阶段进行处理（第二阶段超时）
- 前半部分（判断是否命中）保留，后半部分（取出对应数据）延后s至状态机的第二阶段进行处理（成功）。

##### 读命中逻辑去除

由于在AXI中，我们提前的写成功的信号，也就是当AXI确认开始写入之时就将其标记为已经写成功，然后AXI将独立继续完成相关的操作，但是对于WriteBuffer来说已经写成功。这导致WriteBuffer很难处于占用的情况，即使出现也持续两三个周期，因此理论上来说虽然**存在**命中的可能性但是实际上整个程序都没有出现命中，因此我们将试探性的将其功能去除。最后并未影响功能性，并且获得了比较客观的提升。

#### CacheAXI_Interface屏蔽操作

之前，在CacheAXI_Interface中，我们所有送往AXI的数据都是基于cpu或是Cache的数据，这样会造成通路过长，并且，在之前的设计中，还产生了相应的“转瞬即逝”的时序错误，也就是数据得不到保持造成的错误。因此，在CacheAXI_Interface之中，我们将这一条非常长的关键路径进行屏蔽，也就是在此设立相应的寄存器进行保存，来进行相关的优化。该优化小幅度提升了频率，性价比高。

#### DCache流水

在进行DCache流水的时候，我们顺便将内部的命名格式进行相应的优化，使得其没有那么累赘，让代码更为清晰易懂。优化命名，将累赘的连线和命名以及定义进行改良。向之前的ICache优化版本看齐。

切流水，同样仿照ICache的流水进行相应的切分。具体来说，由于写操作和脏块操作的不同，我们需要再次考虑以下问题。

##### 数据保持

- Cache Ram中第二阶段的Data与Tag都要保存起来，但是这里我们采用选通index地址来进行保持。如果进入了stall，那么ram的地址设为第二流水段，也就是正在处理的流水段；如果没有出现stall，那么ram的地址就设为cpu输入地址。提醒：这样在stall阶段不会出现ram的读写冲突，在非stall情况下才可能出现。
- 其他的写选择、读写使能、数据地址、写数据都按照正常流水法则，非stall就一直取输入，否则遇到stall就保持。

##### Cache Ram的写入

- 读命中：无操作
- 读不命中：Data主存数据写入，Tag实地址写入
- 写不命中：Data主存数据与写数据拼接写入，Tag实地址写入
- 写命中：主存数据与写数据拼接写入，Tag实地址写入

##### RAM读写冲突

采用simple dual port ram，读写同地址进行访问的时候，写优先，下一拍读出的数据为错误数据。具体操作与ICache相似。

##### 暂停

- 读不命中：暂停至数据返回为止
- 需要写入但FIFO满：暂停至FIFO有足够空间为止
- **写不命中**：写不命中且有新的访存提出（由于作者太蠢忘记放在提交版本了，本来性能还能提升的，但是在github上的文件是添加了的）。

##### 写入FIFO

- FIFO写命中时：将FIFO中读命中的数据拼接写数据送入FIFO
- 脏块弹出：将Cache Ram弹出块送入FIFO

#### DCache重构级时序优化（未实现）

在之后的优化之中，我们发现，由于双发射的固有问题，整个旁路会造成非常严重的延时，在寄存器堆的冲突判断之中占有了五层的延时，在这种硬性的限制之下，DCache无法进一步的优化，所以我们必须改变现有的架构。

同时，我们发现ex阶段的延时并不大，也就是说对于整个旁路来说，没有很大的影响，剩下的可能就是mem阶段的高延时了。所以我们做出最终的优化决定：DCache时序跨度升至3级（ex、mem、wb），在ex阶段送入访存请求，从wb阶段送出访存数据。最后一级的延时非常短，对于旁路来说也显得无所谓，因此我们期待此次优化将会给整体的性能带来爆炸性的提升，有望提升主频至100MHZ。

由于时序的改变，我们需要面对一些设计上的以及与cpu交互的问题。

##### 状态机

- Look Up: 将地址传给TLB进行虚实地址转换，将虚地址给予TagRAM当拍即读出Tag，不进行实地址的命中判断。下一阶段进入Fetch Data。
- Fetch Data: 进行命中判断，其余的比较与之前BRAM的操作相同，但是要注意存储第一个状态传过来的数据，避免“转瞬即逝”的时序问题。
- Send Data: 此时，DCache的数据已经准备好了，只剩下输出了。

##### 异常处理

当遇到mem阶段传入异常处理之后，将立刻停止访存的请求，这样可以避免提起对访存的请求，符合规范。

##### Uncached时序

Uncached是不支持流水的，这就意味着，如果我们进行三段的跨度，我们就要做一定的时序变换，将主处理阶段的时序放在mem阶段，但是输出数据要在wb阶段意味着在即将输出的时候必须将ex阶段的访存数据（以及使能）进行保存，然后下一阶段再在ex阶段统一处理。

##### 流水线

由于我们已经实现了DCache的流水，所以我们直接从流水线上进行修改，而不从状态机Cache上进行开发，这样可以方便解决时序导致的数据丢失问题。

##### 最终结果

首先，这样的改动势必是时序改动，是架构上的改动，我们需要考虑非常多的衍生问题，同时这仅仅只能解决mem阶段的关键路径问题，但是却并不能解决ex阶段的关键路径问题。同时我们将访存数据后移一个周期会导致非常多的后遗症以及待解决的问题。

### 第五代Cache

非常抱歉，我们由于功能上的鲁棒性未能达到要求，我们不得不在第四代性能提升版本的Cache上进一步的加入一些设计调整。

#### 设计思想

整体上来说，还是时序上的问题，在时序上我们没有遵守，或者说没有设计能够包括全部的时序。其中高层次问题如下：

- 数据未保持
- 信号未屏蔽
- 互联层次过高

针对该问题，我们将所有接口的鲁棒性都进行调整，增强输出数据的有效性，同时降低对输入信号的有效性依赖，进一步减少各接口之间的耦合性。

#### CacheAXI_Interface数据有效性强化

当前CacheAXI_Interface拥有六个输入通道，对于各个通道，我们将在其尚未发出请求的时候不改变其输出的数据，直到有新的请求。

#### Data Uncached时序统一化

之前，我们将Data Uncached的读写时序分开，写放在ex阶段，而读放在mem阶段，这是偷懒的做法，当我们遇见了先读后写的uncached信号的时候，我们就会出现问题。*quicksort：9fc00d38: 8fa60068 lw a2,104(sp)   9fc00d3c: afa50030 sw a1,48(sp)* 同时，由于当前TLB的引入导致时序的变化，mem阶段我们需要额外判断一个异常导致的停止访存的信号，因此我们需要将整个axi访存阶段全部挪至mem，将读写的处理都放在mem阶段，以保证能够在握手之前打断。

#### Data Uncached时序处理

这里我们一定要小心，很容易出现错误，信号一定要给对。在时序问题上很容易出现大问题，作者本人在后期出现了无数的Uncached相关bug。

##### 如何解决Cached和Uncached交替出现

Uncached data处理完的同时，我们才会进行相应的Cached处理。Cached同理。

##### 如何解决Uncached连续出现

状态机会在第一次Uncached处理结束之后判断是否下一次还是Uncached，然后进行跳转进行相应的处理。

##### 如何解决Uncached读写连续出现

有两种处理办法

- 合并状态机：因为不可能同时处理读写指令将读写状态机都合并成为一个状态，虽然直截了当但是状态机比较复杂。
- 分立状态机（最终采用）：虽然状态机简单，但是互相交融，需要判断对方是否工作。



### Cache命中率

在进行命中率的判定时，发现WriteBuffer从来没有命中过。

#### ICache

由于动态取指的加入，其预取会降低小部分命中率，但是“预取”本身就意味着在不暂停的情况做先一步的取指，因此最后表现反而是提升了命中率。并且在流水线中由于InstBuffer，因取指不命中而暂停的情况会大大减少。实际上动态取值加入提升了50%的程序运行速度。

- crc32：命中177977，总请求178061，命中率99.95%
- bitcount：命中27216，总请求27325，命中率99.60%
- sha：命中103211，总请求103450，命中率99.77%
- quicksort：命中149996，总请求150257，命中率99.83%
- dhrystone：命中37671，总请求37857，命中率99.51%
- bubblesort：命中155851，总请求155942，命中率99.94%
- stringsearch：命中94398，总请求94501，命中率99.89%
- coremark：命中249883，总请求251223，命中率99.47%
- stringcopy：命中9066，总请求9135，命中率99.24%
- selectsort：命中125303，总请求125390，命中率99.93%
- 平均每个程序的命中率：99.71%
- 平均每次取指的命中率：99.77%

#### DCache

- crc32：命中53527，总请求53586，命中率99.89%
- bitcount：命中3976，总请求4002，命中率99.35%
- sha：命中39995，总请求40115，命中率99.70%

- quicksort：命中38349，总请求38747，命中率98.97%
- dhrystone：命中9276，总请求9349，命中率99.22%
- bubblesort：命中61738，总请求61828，命中率99.85%
- stringsearch：命中33167，总请求33322，命中率99.53%
- coremark：命中62573，总请求62660，命中率99.86%
- stringcopy：命中4255，总请求4522，命中率94.10%
- selectsort：命中21735，总请求21827，命中率99.58%
- 平均每个程序的命中率：99.00%
- 平均每次访存的命中率：99.59%

#### 额外思考

##### ICache

理论上来说，ICache已经加上了动态取指，应该是难以优化了。其他的方式显得没有那么必要。

##### DCache

在具体的性能测试中发现，其实程序的数据段根本就不大，在前期不命中之后，程序后期几乎全部命中。这同时意味着，没有什么很好的方法能够解决这种问题，虽然我们可以取巧，在遇到第一个访存的指令时，就开始顺序预取附近的数据，但是这显然不符合我们整体优化的思路（以真实世界为优化场景，以架构优化为优化手段，不取巧，只做有借鉴意义或者突破意义的优化）。



### Cache模块的相关优化：

#### 双发射同时执行两条访存指令（很难做到）

这是多发射处理器必须要做的事情，但是实际上的难度非常高。UltraMIPS的双发射处理器是最简单的双发射六级流水线处理器。因此，在目前设计的Cache中，其实瓶颈在处理器。即使如此，我还是有必要提到这一个方案，以求为读者提供更为广阔的视野和优化方案。

首先，对于该方案，十分建议在乱序多发射处理器中使用，因为乱序多发射处理器一定会具有寄存器重命名以及动态调度，能够解决很多同时执行两条访存指令所带来的问题，但是复杂度还是远远高于之前的结构。

当我们的DCache支持同时两条访存时，我们需要两套几乎相同的判断逻辑以及相应改动的WriteBuffer，同时还会引起更大的数据冲突，并且对于Uncached相关处理也显得更为复杂。CPU也需要处理只有一条不命中的情况。

#### TagV ram采用组合逻辑寄存器DRAM（已实现，但未出现在最终版本之中）

#### Store Buffer（实现但是未出现在最终版本中）

与WriteBuffer类似，但是处理Uncached的写请求。需要注意的是由于Uncached（比如外设），则要注意先后顺序关系，遇到Uncached读要先等Uncached写完成之后再处理读。而且由于其特殊性，不能够保证写进去的值一定就是地址存储的数据，所以读的数据也不能来自Store Buffer。

由于时间非常紧张，虽然在之前的版本中我们将Store Buffer进行了开发但是最后没有将其移至进最终的版本。有小幅度的提升，预计在0.3分左右。

#### Victim Buffer

该部件主要为了解决反复弹出的问题，也就是说，我们在二路组相联DCache中会遇见同一组出现三个地址需要反复访存的情况。这种情况下我们加入该部件，去存储所有弹出的脏块，使其在其中停留一阵，以防止上述情况出现。

#### 双InstBuffer

在比赛结束当天，浙大的同学提出了一个想法，本人觉得非常巧妙，也进行了一定的思考和拓展。

我们采用双InstBuffer分别存储跳转与不跳转情况下的指令。当然我们取指还是主要以分支预测预测的为主，但是我们在InstBuffer满的时候，也就是ICache空闲的时候可以进行命中失败的指令取出相关处理。当我们发现没有命中的时候，将译码发射的InstBuffer迅速切换到另一个InstBuffer，使得只损失译码当前所在的指令，之后便可以立刻恢复正常的流水。这样我们可以大大降低分支预测失败的可能。但是仔细思考之后，这并非易事。

首先，双InstBuffer如何切换？

- 我们需要接收flush信号，也就是分支预测失败信号。接受之后切换InstBuffer进行取指。
- 由于分支预测失败，之前的InstBuffer指令都需要进行清空，之后的指令也得进切换后的InstBuffer
- 很明显，无论是入队还是出队，都需要判断逻辑增加一层延时

其次，ICache取指会遇见一些问题

- 为了防止分支预测失败，ICache会去取指一些可能根本短时间不需要用到的指令，如果没有命中，那么就会导致几百个周期的延时，值得吗？但是对于这个是有办法解决的，jal等必须跳转的指令不可能会分支预测失败，那么没必要准备预测失败的指令，对于条件跳转，一般来说很大几率在将来会预测失败，因此就算ICache取指不命中，取进来之后迟早会用到。
- 如果面对很多次不同的跳转，两个InstBuffer是不是不够用了？可以做一些妥协，如果发现最近的一次分支预测成功，那么就清除失败的那个InstBuffer，取指永远跟着主InstBuffer，副InstBuffer**只存**最近将要遇到的跳转指令预测失败的后四条（自定义）指令。

总结：对其做了比较深刻的分析后，本人认为，在双发射结构上，这个方案是值得的，不仅非常特别而且也非常有价值。当然，预测对于更加高的发射级别，这个架构更有用。



